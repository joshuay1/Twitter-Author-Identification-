{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "\n",
    "np.random.seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process: Data reading is completed.\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/train_data.csv')\n",
    "train_data.drop(train_data.columns[0], axis=1,inplace=True)\n",
    "dev_data = pd.read_csv('data/dev_data.csv')\n",
    "dev_data.drop(dev_data.columns[0], axis=1,inplace=True)\n",
    "\n",
    "#data = pd.read_csv('data/all_data.csv')\n",
    "#data.drop(data.columns[0], axis=1,inplace=True)\n",
    "print('Process: Data reading is completed.')\n",
    "\n",
    "#train_data['author_Id'] = '__label__'+train_data['author_Id'].astype(str)\n",
    "train_data['is_retweet'] = 1*train_data['is_retweet']\n",
    "train_data['is_modified_retweet'] = 1*train_data['is_modified_retweet']\n",
    "\n",
    "#dev_data['author_Id'] = '__label__'+dev_data['author_Id'].astype(str)\n",
    "dev_data['is_retweet'] = 1*dev_data['is_retweet']\n",
    "dev_data['is_modified_retweet'] = 1*dev_data['is_modified_retweet']\n",
    "\n",
    "#data['is_retweet'] = 1*data['is_retweet']\n",
    "#data['is_modified_retweet'] = 1*data['is_modified_retweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev_data.to_csv('fastText/dev.txt', index=False, sep=' ', header=None)\n",
    "#train_data.to_csv('fastText/train.txt', index=False, sep=' ', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246699 246699 82233 82233\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#labelencoder = LabelEncoder()\n",
    "#train_data['author_Id_label'] = train_data['author_Id']\n",
    "#dev_data['author_Id_label'] = dev_data['author_Id'])\n",
    "#print(train_data.author_Id.max())\n",
    "#print(dev_data.author_Id.max())\n",
    "#print(train_data.author_Id_label.max())\n",
    "#print(dev_data.author_Id_label.max())\n",
    "#print(train_data.author_Id.head())\n",
    "#print(train_data.author_Id_label.head())\n",
    "#print(dev_data.author_Id.head())\n",
    "#print(dev_data.author_Id_label.head())\n",
    "\n",
    "y_train = to_categorical(train_data['author_Id'])\n",
    "X_train = train_data['tweet']\n",
    "#X_train = train_data.drop('author_Id', axis=1)\n",
    "\n",
    "y_dev = to_categorical(dev_data['author_Id'])\n",
    "X_dev = dev_data['tweet']\n",
    "#X_dev = dev_data.drop('author_Id', axis=1)\n",
    "\n",
    "#y = to_categorical(data['author_Id'])\n",
    "#X = data['tweet']\n",
    "\n",
    "print(len(y_train),len(X_train),len(y_dev),len(X_dev))\n",
    "#print(X.shape)\n",
    "#print(y.shape)\n",
    "#print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?!')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df:\n",
    "        if type(doc) == float:\n",
    "            print(doc)\n",
    "            doc = str(doc)\n",
    "        doc = preprocess(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     0      0      0 ...   5096    148     77]\n",
      " [     0      0      0 ...    295  19225   5474]\n",
      " [     0      0      0 ...  11757  50442    911]\n",
      " ...\n",
      " [     0      0      0 ...  40561  86732  21234]\n",
      " [     0      0      0 ...    403   2281  24621]\n",
      " [     0      0      0 ... 157601  54855     45]]\n"
     ]
    }
   ],
   "source": [
    "min_count = 3\n",
    "\n",
    "docs_train = create_docs(X_train)\n",
    "tokenizer = Tokenizer(lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs_train)\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs_train)\n",
    "docs_train = tokenizer.texts_to_sequences(docs_train)\n",
    "\n",
    "maxlen = 256\n",
    "\n",
    "docs_train = pad_sequences(sequences=docs_train, maxlen=maxlen)\n",
    "\n",
    "print(docs_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ... 27682     1 37924]\n",
      " [    0     0     0 ...   808  1967   758]\n",
      " [    0     0     0 ... 10768  8477  1555]\n",
      " ...\n",
      " [    0     0     0 ... 27723  4100   465]\n",
      " [    0     0     0 ...  7111  5719   136]\n",
      " [    0     0     0 ...  1470   211 60103]]\n"
     ]
    }
   ],
   "source": [
    "min_count = 3\n",
    "\n",
    "docs_dev = create_docs(X_dev)\n",
    "tokenizer = Tokenizer(lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs_dev)\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs_dev)\n",
    "docs_dev = tokenizer.texts_to_sequences(docs_dev)\n",
    "\n",
    "maxlen = 128\n",
    "\n",
    "docs_dev = pad_sequences(sequences=docs_dev, maxlen=maxlen)\n",
    "\n",
    "print(docs_dev[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220177\n",
      "85097\n"
     ]
    }
   ],
   "source": [
    "#print(y_train[:100])\n",
    "print(np.max(docs_train))\n",
    "print(np.max(docs_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim = 220178\n"
     ]
    }
   ],
   "source": [
    "input_dim = np.max(docs_train)+1\n",
    "print('input_dim = ' + str(input_dim))\n",
    "embedding_dims = 20\n",
    "\n",
    "def create_model(embedding_dims=20, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_dev, y_train, y_dev = train_test_split(docs, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 246699 samples, validate on 82233 samples\n",
      "Epoch 1/10\n",
      "246699/246699 [==============================] - 435s 2ms/step - loss: 8.9960 - acc: 7.6206e-04 - val_loss: 8.9305 - val_acc: 8.8772e-04\n",
      "Epoch 2/10\n",
      "   192/246699 [..............................] - ETA: 4:11:27 - loss: 8.9535 - acc: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.135469). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n",
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.178986). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246699/246699 [==============================] - 416s 2ms/step - loss: 8.8735 - acc: 0.0011 - val_loss: 8.8839 - val_acc: 0.0014\n",
      "Epoch 3/10\n",
      "   128/246699 [..............................] - ETA: 44:33 - loss: 8.8012 - acc: 0.0000e+00  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.122680). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246699/246699 [==============================] - 393s 2ms/step - loss: 8.7622 - acc: 0.0016 - val_loss: 8.8147 - val_acc: 0.0014\n",
      "Epoch 4/10\n",
      "246699/246699 [==============================] - 389s 2ms/step - loss: 8.6474 - acc: 0.0024 - val_loss: 8.7523 - val_acc: 0.0022\n",
      "Epoch 5/10\n",
      "246699/246699 [==============================] - 392s 2ms/step - loss: 8.5454 - acc: 0.0034 - val_loss: 8.6912 - val_acc: 0.0026\n",
      "Epoch 6/10\n",
      "246699/246699 [==============================] - 955s 4ms/step - loss: 8.3495 - acc: 0.0066 - val_loss: 8.5808 - val_acc: 0.0039\n",
      "Epoch 8/10\n",
      "246699/246699 [==============================] - 884s 4ms/step - loss: 8.2591 - acc: 0.0086 - val_loss: 8.5386 - val_acc: 0.0045\n",
      "Epoch 9/10\n",
      "246699/246699 [==============================] - 784s 3ms/step - loss: 8.1741 - acc: 0.0106 - val_loss: 8.5055 - val_acc: 0.0051\n",
      "Epoch 10/10\n",
      "246699/246699 [==============================] - 565s 2ms/step - loss: 8.0927 - acc: 0.0134 - val_loss: 8.4754 - val_acc: 0.0051\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "model = create_model()\n",
    "hist = model.fit(docs_train, y_train,\n",
    "                 batch_size=64,\n",
    "                 validation_data=(docs_dev, y_dev),\n",
    "                 verbose=1,\n",
    "                 epochs=epochs,\n",
    "                 callbacks=[EarlyStopping(patience=3, monitor='val_loss', min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-504cb361608b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fastTextmodel_weights.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fastTextmodel.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save_weights('fastTextmodel_weights.h5')\n",
    "model.save('fastTextmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "#model = MultinomialNB()\n",
    "#model = model.fit(text_bow_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 246699 samples, validate on 82233 samples\n",
      "Epoch 1/5\n",
      "246699/246699 [==============================] - 590s 2ms/step - loss: 7.4871 - acc: 0.0424 - val_loss: 8.3638 - val_acc: 0.0068\n",
      "Epoch 2/5\n",
      "   128/246699 [..............................] - ETA: 1:44:12 - loss: 7.4041 - acc: 0.0312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.150697). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246699/246699 [==============================] - 657s 3ms/step - loss: 7.4308 - acc: 0.0457 - val_loss: 8.3595 - val_acc: 0.0070\n",
      "Epoch 3/5\n",
      "246699/246699 [==============================] - 597s 2ms/step - loss: 7.3753 - acc: 0.0490 - val_loss: 8.3630 - val_acc: 0.0069\n",
      "Epoch 4/5\n",
      "246699/246699 [==============================] - 528s 2ms/step - loss: 7.3204 - acc: 0.0527 - val_loss: 8.3611 - val_acc: 0.0071\n",
      "Epoch 5/5\n",
      "246699/246699 [==============================] - 603s 2ms/step - loss: 7.2663 - acc: 0.0556 - val_loss: 8.3588 - val_acc: 0.0070\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(docs_train, y_train,\n",
    "                 batch_size=64,\n",
    "                 validation_data=(docs_dev, y_dev),\n",
    "                 verbose=1,\n",
    "                 epochs=5,\n",
    "                 callbacks=[EarlyStopping(patience=3, monitor='val_loss', min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voodoo",
   "language": "python",
   "name": "voodoo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
